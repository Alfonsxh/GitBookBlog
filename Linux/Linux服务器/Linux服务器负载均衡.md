# Linux服务器负载均衡

这篇主要是了解一下早年前辈们 **如何设计服务器，应对高并发网络请求**，实际上可能不是这样操作，但是 **这种解决问题的思路十分重要**。

## 服务器集群

在远古时期，面对这种高并发的网络场景，大部分策略是 **堆硬件属性**，一个CPU不行，用多个，带宽不行，加带宽。最终的处理逻辑仍是在一台服务器上。

这种处理方式的结果，虽然在程序开发的角度上讲，十分简单，只需要考虑一台服务器上的应用。但在 **性能**、**性价比**、**可伸缩性**、**高可用性** 上，完全没有体现。

最理想的替代方案是，通过多台服务器的组合，联合提供服务，最前端由负载均衡服务器处理请求，将请求推给空闲的服务器进行处理。

集群的方式在 **可伸缩性**、**高可用性** 上的优势明显。

## 服务器集群的体系结构

![服务器集群架构图](/Image/Linux服务器/Linux服务器负载均衡/服务器集群架构图.jpg)

主要由三部分组成：

- **负载调度器(load balance)**
  - 它是整个集群对外面的前端机，负责将客户的请求发送到一组服务器上执行，而客户认为服务是来自一个IP地址（我们可称之为虚拟IP地址）上的。
  - **负载调度器** 是客户端请求的唯一路口，通过它将请求 **根据不同的调度策略分发到连接的真实服务器上**。
  - **负载调度器** 也可以是多台，以 **降低高并发场景下对单台负载调度器的请求压力**。
- **服务器池(server pool)**
  - 是一组真正执行客户请求的服务器。
  - 服务器节点的数量是可变的。
  - 当系统收到的请求数量已超过所有服务器节点的处理能力时，可以通过增加节点来满足。
  - **理论上，系统的处理能力随服务器池中的服务器节点数量的增加而线性增加**。
- **共享存储(shared storage)**
  - 它为服务器池提供一个共享的存储区，这样很容易使得服务器池拥有相同的内容，提供相同的服务。
  - 通常是 **数据库**、**分布式文件系统**。
  - **分布式文件系统** 可以为所有的服务器提供共享存储服务，提供了良好的 **可伸缩性** 和 **高可用性**。
  - 对于数据一致性的问题，需要使用 **分布式锁管理器**。

> **为何采用层次的体系结构？**
>
> 层次的结构使得层与层之间相互独立，方便扩展。可以参考TCP/IP协议。
>
> **为何采用共享存储？**
>
> 如果不采用的话，需要将相同的内容都拷贝到每一台提供服务的节点上，这显然不是好的解决方案，一来是数据量的问题，二来是数据同步的问题。
> 采用 **分布式存储** 的方式，将数据存储服务交由 **分布式存储** 来处理，每台 **分布式存储服务器** 上存储的数据不会太多，数据的管理不需要 **服务器池中的节点** 来处理，它只需要像本地磁盘上使用数据一样进行操作即可。
> 目前为了应对这种局域网存储服务，有了许多提升带宽的技术：**光纤通道（Fiber Channel）**、**共享SCSI（Shared SCSI）**、**InfiniBand**。

## IP负载均衡技术

负载均衡的目的主要在于，使得服务器池中的服务器能够尽可能的分摊请求的压力。基于IP层的负载均衡技术主要有三种：**通过NAT实现虚拟服务器(VS/NAT)**、**通过IP隧道实现虚拟服务器(VS/TUN)**、**通过直接路由实现虚拟服务器(VS/DR)**。

这三种方式，通过客户端与服务器池中间的 **负载均衡服务器进行连接**。

### 通过NAT实现虚拟服务器(VS/NAT)

![vs-nat](/Image/Linux服务器/Linux服务器负载均衡/vs-nat.jpg)

这种方式和运营商使用的解决IP资源短缺所使用的方式一样，采用了 **网络地址转换（NAT, Network Address Translation)** 方式，在负载均衡服务器接受到客户端请求时，将请求转发到一台真实的服务器上，每台服务器处理的流程都一样，对文件的修改可以放在本地(不过需要在不同的服务器之间进行同步)，或者使用分布式的文件系统。

具体操作如下：

开始请求的地址如下

|SOURCE|DEST|
|:---:|:---:|
|202.100.1.2:3456|202.103.106.5:80|

经过负载均衡处理过后，将目标地址通过 **NAT** 映射到服务器池所在的虚拟地址上。

|SOURCE|DEST|
|:---:|:---:|
|202.100.1.2:3456|172.16.0.3:8000|

服务器返回的报文地址如下

|SOURCE|DEST|
|:---:|:---:|
|172.16.0.3:8000|202.100.1.2:3456|

响应报文会再一次经过负载均衡服务器处理，将报文的源地址改写回服务器池的开放地址

|SOURCE|DEST|
|:---:|:---:|
|202.103.106.5:80|202.100.1.2:3456|

**NAT** 的方式，**客户端的请求与响应报文都得通过负载均衡服务器，这无形中给负载均衡服务器带来了压力。这使得服务器集群的吞吐量，很大程度上受限于负载均衡服务器的带宽**。

### 通过IP隧道实现虚拟服务器(VS/TUN)

![vs-tun](/Image/Linux服务器/Linux服务器负载均衡/vs-tun.jpg)

使用 **IP隧道(IP tunneling)** 的方式，和 **NAT** 方式相比，响应的报文不用经过负载均衡服务器的再一次转发。

当客户端的请求到达负载均衡服务器后，负载均衡服务器根据服务器池的服务器负载情况，选择一台合适的服务器，**将请求的报文封装到另一个报文中，然后在封装后的报文发送给目标服务器处理**。

![vs-tun-flow](/Image/Linux服务器/Linux服务器负载均衡/vs-tun-flow.jpg)

在服务器解开封装后的报文后，客户端的地址也就知道了，可以不通过负载均衡服务器，直接将响应的报文发送给目标客户端。

### 通过直接路由实现虚拟服务器(VS/DR)

![vs-dr](/Image/Linux服务器/Linux服务器负载均衡/vs-dr.jpg)

和 **VS/TUN(Virtual Server via Direct Routing)** 的方式类似，不过这里是将请求的 **mac地址** 修改为目标服务器的mac地址。再将修改后的报文通过路由器进行转发。

![vs-dr-flow](/Image/Linux服务器/Linux服务器负载均衡/vs-dr-flow.jpg)

### 三种方式对比

- **VS/NAT**
  - NAT方式可以适配任何支持 **TCP/IP** 的操作系统
  - 只需要一个ip配置在负载均衡服务器上，服务器池中的服务器可以拥有私有的ip地址
  - 缺点在于，请求和响应都需要通过负载均衡服务器，使得负载均衡服务器变成了整个系统的瓶颈。
- **VS/TUN**
  - 负载均衡服务器 **只需处理客户端的请求报文**，服务器将响应报文直接返回给客户端。
  - 负载均衡服务器不是整个系统的瓶颈，即使负载均衡服务器的网络带宽有限，也能处理大量的请求响应。
  - 缺点在于，对服务器的操作系统有要求，需要支持 **IP tunneling** 技术。
- **VS/DR**
  - 负载均衡服务器不是整个系统的瓶颈。也没有 **IP隧道** 的开销。
  - 需要 **服务器与负载均衡服务器的一块网卡在同一网络内**。

## 参考

- [Linux服务器集群系统（二）](http://www.linuxvirtualserver.org/zh/lvs2.html)
